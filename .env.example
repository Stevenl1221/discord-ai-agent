DISCORD_TOKEN=
GUILD_ID=

# Optional MCP-context enrichment (no direct bot<->MCP integration)
USE_MCP_CONTEXT=true
MCP_CONTEXT7_URL=http://localhost:8888/docs
MCP_DISCORD_DOCS_URL=http://localhost:9999/docs

# Models (names for your local stack)
# For Ollama examples:
# TEXT_MODEL_NAME=llama3.1
# EMBED_MODEL_NAME=mxbai-embed-large
TEXT_MODEL_NAME=local-llm
EMBED_MODEL_NAME=local-embed
VISION_MODEL_NAME=local-vision

# Toggles
USE_FAISS=true

# Optional local endpoints for LLM and vision
LLM_BASE_URL=http://localhost:11434
VISION_BASE_URL=http://localhost:5000

# Optional HTTP timeouts (seconds)
# Increase if your model is slow to respond
# LLM_TIMEOUT=60
# EMBED_TIMEOUT=30

# Generation token limits (sensible defaults)
# SPEAK_MAX_TOKENS=256
# CREATE_MAX_TOKENS=384
# SUMMARIZE_MAX_TOKENS=192
# SUMMARIZE_MSG_MAX_CHARS=160
# SUMMARIZE_TOTAL_MAX_CHARS=3000
# SUMMARIZE_FAST=false

# RAG and prompt size tuning
# RAG_K=3
# RAG_SNIPPET_MAX_CHARS=240
# STYLE_MAX_CHARS=1000

# Optional: prewarm LLM on startup (non-blocking)
# PREWARM_LLM=false

# Optional base system prompt prepended to all LLM prompts
# BASE_SYSTEM_PROMPT=
