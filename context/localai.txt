========================
CODE SNIPPETS
========================
TITLE: Basic Bash Installation
DESCRIPTION: Installs LocalAI using a simple curl command. This is the most straightforward way to get started.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#_snippet_0

LANGUAGE: bash
CODE:
```
# Basic installation
curl https://localai.io/install.sh | sh
```

----------------------------------------

TITLE: LocalAGI Quick Start with Docker Compose
DESCRIPTION: This snippet demonstrates how to clone the LocalAGI repository and start the application using Docker Compose. It includes configurations for CPU, NVIDIA GPU, and Intel GPU environments, as well as options for specifying models and multimodal/image models.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#_snippet_11

LANGUAGE: bash
CODE:
```
git clone https://github.com/mudler/LocalAGI
cd LocalAGI

# CPU setup (default)
docker compose up

# NVIDIA GPU setup
docker compose -f docker-compose.nvidia.yaml up

# Intel GPU setup (for Intel Arc and integrated GPUs)
docker compose -f docker-compose.intel.yaml up

# Start with a specific model (see available models in models.localai.io, or localai.io to use any model in huggingface)
MODEL_NAME=gemma-3-12b-it docker compose up

# NVIDIA GPU setup with custom multimodal and image models
MODEL_NAME=gemma-3-12b-it \
MULTIMODAL_MODEL=minicpm-v-2_6 \
IMAGE_MODEL=flux.1-dev-ggml \
docker compose -f docker-compose.nvidia.yaml up
```

----------------------------------------

TITLE: LocalAI Docker Compose Setup
DESCRIPTION: This section outlines the steps to set up LocalAI using Docker Compose. It includes cloning the repository, copying models, optionally configuring environment variables, and starting the service. It also provides API examples for listing models and generating completions.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#_snippet_11

LANGUAGE: bash
CODE:
```
# Clone LocalAI
git clone https://github.com/go-skynet/LocalAI

cd LocalAI

# (Optional) Checkout a specific LocalAI tag
# git checkout -b build <TAG>

# Copy your models to the models directory
cp your-model.gguf models/

# (Optional) Edit the .env file to set parameters like context size and threads
# vim .env

# Start with Docker Compose
docker compose up -d --pull always
# Or build the images with:
# docker compose up -d --build

# Now the API is accessible at localhost:8080
curl http://localhost:8080/v1/models
# {"object":"list","data":[{"id":"your-model.gguf","object":"model"}]}

curl http://localhost:8080/v1/completions -H "Content-Type: application/json" -d '{
     "model": "your-model.gguf",
     "prompt": "A long time ago in a galaxy far, far away",
     "temperature": 0.7
   }'
```

----------------------------------------

TITLE: LocalAI Installation Script
DESCRIPTION: Installs LocalAI using a one-line curl command. This is the fastest way to get started with LocalAI.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/overview.md#_snippet_0

LANGUAGE: bash
CODE:
```
curl https://localai.io/install.sh | sh
```

----------------------------------------

TITLE: Build and Run LocalAI on Mac (M1/M2/M3)
DESCRIPTION: A comprehensive example for building LocalAI on a Mac, including installing dependencies, cloning the repo, building the binary, downloading a model, setting up a template, installing a backend, and running the server. It also shows how to interact with the API.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#_snippet_4

LANGUAGE: shell
CODE:
```
# install build dependencies
brew install abseil cmake go grpc protobuf wget protoc-gen-go protoc-gen-go-grpc

# clone the repo
git clone https://github.com/go-skynet/LocalAI.git

cd LocalAI

# build the binary
make build

# Download phi-2 to models/
wget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q2_K.gguf -O models/phi-2.Q2_K

# Use a template from the examples
cp -rf prompt-templates/ggml-gpt4all-j.tmpl models/phi-2.Q2_K.tmpl

# Install the llama-cpp backend
./local-ai backends install llama-cpp

# Run LocalAI
./local-ai --models-path=./models/ --debug=true

# Now API is accessible at localhost:8080
curl http://localhost:8080/v1/models

curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{
     "model": "phi-2.Q2_K",
     "messages": [{"role": "user", "content": "How are you?"}],
     "temperature": 0.9 
   }'
```

----------------------------------------

TITLE: Run LocalAI with Docker (AIO Images)
DESCRIPTION: Launches LocalAI containers with pre-downloaded models for various hardware configurations (CPU, NVIDIA, Intel, AMD).

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#_snippet_8

LANGUAGE: dockerfile
CODE:
```
# AIO Images (pre-downloaded models):

# CPU version
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu

# NVIDIA CUDA 12 version
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-12

# NVIDIA CUDA 11 version
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-aio-gpu-nvidia-cuda-11

# Intel GPU version
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-gpu-intel

# AMD GPU version
docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-aio-gpu-hipblas
```

----------------------------------------

TITLE: Run Model via Configuration File URI
DESCRIPTION: Starts LocalAI using a model configuration file hosted online.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#_snippet_4

LANGUAGE: bash
CODE:
```
local-ai run https://gist.githubusercontent.com/.../phi-2.yaml
```

----------------------------------------

TITLE: Load Models with LocalAI CLI
DESCRIPTION: Demonstrates various ways to load models using the `local-ai run` command, including from the model gallery, Hugging Face, Ollama, Gists, and OCI registries.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#_snippet_9

LANGUAGE: bash
CODE:
```
# From the model gallery (see available models with `local-ai models list`, in the WebUI from the model tab, or visiting https://models.localai.io)
local-ai run llama-3.2-1b-instruct:q4_k_m
# Start LocalAI with the phi-2 model directly from huggingface
local-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf
# Install and run a model from the Ollama OCI registry
local-ai run ollama://gemma:2b
# Run a model from a configuration file
local-ai run https://gist.githubusercontent.com/.../phi-2.yaml
# Install and run a model from a standard OCI registry (e.g., Docker Hub)
local-ai run oci://localai/phi-2:latest
```

----------------------------------------

TITLE: Run LocalAI with Docker (Intel GPU - oneAPI)
DESCRIPTION: Launches a LocalAI container with Intel GPU support using oneAPI, exposing the API on port 8080.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#_snippet_6

LANGUAGE: dockerfile
CODE:
```
# Intel GPU Images (oneAPI):
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-gpu-intel
```

----------------------------------------

TITLE: Run LocalAI with Docker (Vulkan GPU)
DESCRIPTION: Launches a LocalAI container with Vulkan GPU support, exposing the API on port 8080.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#_snippet_7

LANGUAGE: dockerfile
CODE:
```
# Vulkan GPU Images:
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-gpu-vulkan
```

----------------------------------------

TITLE: Run Model via Hugging Face URI
DESCRIPTION: Starts LocalAI using a model file hosted on Hugging Face.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#_snippet_2

LANGUAGE: bash
CODE:
```
local-ai run huggingface://TheBloke/phi-2-GGUF/phi-2.Q8_0.gguf
```

----------------------------------------

TITLE: Run LocalAI with Docker (NVIDIA Jetson ARM64)
DESCRIPTION: Launches a LocalAI container optimized for NVIDIA Jetson devices (ARM64), requiring the NVIDIA container toolkit.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#_snippet_4

LANGUAGE: dockerfile
CODE:
```
# NVIDIA Jetson (L4T) ARM64
# First, you need to have installed the nvidia container toolkit: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-with-ap
docker run -ti --name local-ai -p 8080:8080 --runtime nvidia --gpus all localai/localai:latest-nvidia-l4t-arm64
```

----------------------------------------

TITLE: Install LocalAI with Homebrew (macOS)
DESCRIPTION: Installs LocalAI on macOS using the Homebrew package manager. Note that the Homebrew formula may have fewer options than the bash script.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#_snippet_10

LANGUAGE: bash
CODE:
```
brew install localai
```

----------------------------------------

TITLE: Run LocalAI with Docker (CPU Only)
DESCRIPTION: Launches a LocalAI container using the CPU-only image, exposing the API on port 8080.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#_snippet_1

LANGUAGE: dockerfile
CODE:
```
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest
```

----------------------------------------

TITLE: Run LocalAI with Docker (AMD GPU - ROCm)
DESCRIPTION: Launches a LocalAI container with AMD GPU support using ROCm, exposing the API on port 8080.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#_snippet_5

LANGUAGE: dockerfile
CODE:
```
# AMD GPU Images (ROCm):
docker run -ti --name local-ai -p 8080:8080 --device=/dev/kfd --device=/dev/dri --group-add=video localai/localai:latest-gpu-hipblas
```

----------------------------------------

TITLE: Docker Deployment with Local Models
DESCRIPTION: Demonstrates how to run LocalAI using Docker, mounting a local directory containing model files. Includes an example of testing the API endpoint.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#_snippet_6

LANGUAGE: bash
CODE:
```
# Prepare the models into the `models` directory
mkdir models

# Copy your models to the directory
cp your-model.gguf models/

# Run the LocalAI container
docker run -p 8080:8080 -v $PWD/models:/models -ti --rm quay.io/go-skynet/local-ai:latest --models-path /models --context-size 700 --threads 4

# Expected output:
# ┌───────────────────────────────────────────────────┐
# │                   Fiber v2.42.0                   │
# │               http://127.0.0.1:8080               │
# │       (bound on host 0.0.0.0 and port 8080)       │
# │                                                   │
# │ Handlers ............. 1  Processes ........... 1 │
# │ Prefork ....... Disabled  PID ................. 1 │
# └───────────────────────────────────────────────────┘

# Test the endpoint with curl
curl http://localhost:8080/v1/completions -H "Content-Type: application/json" -d '{
     "model": "your-model.gguf",
     "prompt": "A long time ago in a galaxy far, far away",
     "temperature": 0.7
   }'
```

----------------------------------------

TITLE: Install Base Model with Files and Overrides
DESCRIPTION: Demonstrates installing a 'base' model, providing custom files with SHA256 checksums, and applying configuration overrides. This is useful for models not found in the gallery or for custom model setups.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#_snippet_16

LANGUAGE: bash
CODE:
```
curl $LOCALAI/models/apply -H "Content-Type: application/json" -d '{
     "url": "github:mudler/LocalAI/gallery/base.yaml@master",
     "name": "model-name",
     "files": [
        {
            "uri": "<URL>",
            "sha256": "<SHA>",
            "filename": "model"
        }
     ]
   }'
```

----------------------------------------

TITLE: Run LocalAI with Docker (NVIDIA GPU - CUDA 11.7)
DESCRIPTION: Launches a LocalAI container with NVIDIA GPU support for CUDA 11.7, exposing the API on port 8080.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#_snippet_3

LANGUAGE: dockerfile
CODE:
```
# CUDA 11.7
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-11
```

----------------------------------------

TITLE: LocalAI Installation from Binary (macOS)
DESCRIPTION: Instructions for installing LocalAI from a binary release on macOS, including how to handle security prompts related to unidentified developers.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#_snippet_13

LANGUAGE: bash
CODE:
```
If installing on macOS, you might encounter a message saying:

> "local-ai-git-Darwin-arm64" (or the name you gave the binary) can't be opened because Apple cannot check it for malicious software.

Hit OK, then go to Settings > Privacy & Security > Security and look for the message:

> "local-ai-git-Darwin-arm64" was blocked from use because it is not from an identified developer.

Press "Allow Anyway."
```

----------------------------------------

TITLE: Install Model from Gallery
DESCRIPTION: Installs a model from the LocalAI gallery without running it immediately.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#_snippet_1

LANGUAGE: bash
CODE:
```
local-ai models install hermes-2-theta-llama-3-8b
```

----------------------------------------

TITLE: Basic LocalAI Installation
DESCRIPTION: Installs LocalAI using a curl command to download and execute the installation script.

SOURCE: https://github.com/mudler/localai/blob/master/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
# Basic installation
curl https://localai.io/install.sh | sh
```

----------------------------------------

TITLE: Run LocalAI with Docker (NVIDIA GPU - CUDA 12.0)
DESCRIPTION: Launches a LocalAI container with NVIDIA GPU support for CUDA 12.0, exposing the API on port 8080.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/quickstart.md#_snippet_2

LANGUAGE: dockerfile
CODE:
```
# CUDA 12.0
docker run -ti --name local-ai -p 8080:8080 --gpus all localai/localai:latest-gpu-nvidia-cuda-12
```

----------------------------------------

TITLE: Run Hugo Server Locally
DESCRIPTION: Starts a local Hugo development server to preview the website. This command requires an extended version of Hugo to be installed.

SOURCE: https://github.com/mudler/localai/blob/master/docs/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
hugo server
```

----------------------------------------

TITLE: LocalAI Docker Quick Start
DESCRIPTION: Starts LocalAI using Docker, exposing port 8080. This command uses the latest CPU-compatible image.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/overview.md#_snippet_1

LANGUAGE: bash
CODE:
```
docker run -p 8080:8080 --name local-ai -ti localai/localai:latest-aio-cpu
```

----------------------------------------

TITLE: PyTorch and Intel Extension Setup
DESCRIPTION: This snippet shows the necessary package installations for PyTorch with Intel XPU support and the corresponding Intel Extension for PyTorch. It includes an extra index URL for PyTorch extensions.

SOURCE: https://github.com/mudler/localai/blob/master/backend/python/common/template/requirements-intel.txt#_snippet_0

LANGUAGE: bash
CODE:
```
--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
intel-extension-for-pytorch==2.3.110+xpu
torch==2.3.1+cxx11.abi
oneccl_bind_pt==2.3.100+xpu
```

----------------------------------------

TITLE: Install Dependencies on Debian/Ubuntu
DESCRIPTION: Installs Go, make, and protobuf-compiler-grpc on Debian-based systems. It also includes commands to install Go protobuf components.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#_snippet_1

LANGUAGE: bash
CODE:
```
apt install golang make protobuf-compiler-grpc

go install google.golang.org/protobuf/cmd/protoc-gen-go@v1.34.2
go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@1958fcbe2ca8bd93af633f11e97d44e567e945af
```

----------------------------------------

TITLE: LocalAI Configuration Loading Example
DESCRIPTION: Example log output showing LocalAI loading environment variables from an .env file and setting logging levels.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/distributed_inferencing.md#_snippet_6

LANGUAGE: go
CODE:
```
1:06AM INF loading environment variables from file envFile=.env
1:06AM INF Setting logging to info
{"level":"INFO","time":"2024-05-19T01:06:01.794+0200","caller":"config/config.go:288","message":"connmanager disabled\n"}
```

----------------------------------------

TITLE: Setting up LocalAI Development Environment
DESCRIPTION: Steps to clone the repository, install dependencies, build, and run LocalAI locally. Requires Git and Golang 1.21+.

SOURCE: https://github.com/mudler/localai/blob/master/CONTRIBUTING.md#_snippet_0

LANGUAGE: bash
CODE:
```
git clone https://github.com/go-skynet/LocalAI.git
cd LocalAI
make build
./localai
```

----------------------------------------

TITLE: Preloading Models in LocalAI
DESCRIPTION: Demonstrates how to configure LocalAI to automatically download and preload models from a gallery before starting the API. This example shows how to load 'gpt4all-j' and alias it as 'gpt-3.5-turbo'.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/whats-new.md#_snippet_41

LANGUAGE: bash
CODE:
```
PRELOAD_MODELS=[{"url": "github:go-skynet/model-gallery/gpt4all-j.yaml", "name": "gpt-3.5-turbo"}]
```

----------------------------------------

TITLE: PyTorch and Intel Extension Setup
DESCRIPTION: This snippet shows how to configure pip to install Intel's optimized PyTorch extension and related libraries. It specifies the extra index URL for PyTorch extensions and lists the exact versions for PyTorch, Intel Extension for PyTorch, andoneccl_bind_pt.

SOURCE: https://github.com/mudler/localai/blob/master/backend/python/faster-whisper/requirements-intel.txt#_snippet_0

LANGUAGE: bash
CODE:
```
--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
intel-extension-for-pytorch==2.3.110+xpu
torch==2.3.1+cxx11.abi
oneccl_bind_pt==2.3.100+xpu
```

----------------------------------------

TITLE: Install Model from Gallery with Repository
DESCRIPTION: Installs a model from a gallery by specifying both the gallery repository and the model name. The format is `<GALLERY>@<MODEL_NAME>`.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#_snippet_9

LANGUAGE: bash
CODE:
```
LOCALAI=http://localhost:8080
curl $LOCALAI/models/apply -H "Content-Type: application/json" -d '{
     "id": "<GALLERY>@<MODEL_NAME>"
   }'
```

----------------------------------------

TITLE: Accelerate Library Installation
DESCRIPTION: Installs the Accelerate library from Hugging Face, which simplifies the process of running PyTorch training scripts on various distributed setups (multi-GPU, TPUs, etc.).

SOURCE: https://github.com/mudler/localai/blob/master/backend/python/bark/requirements-hipblas.txt#_snippet_3

LANGUAGE: shell
CODE:
```
accelerate
```

----------------------------------------

TITLE: Helm Chart Installation for LocalAI
DESCRIPTION: Installs LocalAI using a Helm chart, allowing for customization via a values.yaml file.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/kubernetes.md#_snippet_2

LANGUAGE: bash
CODE:
```
# Install the helm repository
helm repo add go-skynet https://go-skynet.github.io/helm-charts/
# Update the repositories
helm repo update
# Get the values
helm show values go-skynet/local-ai > values.yaml

# Edit the values if needed
# vim values.yaml ...

# Install the helm chart
helm install local-ai go-skynet/local-ai -f values.yaml
```

----------------------------------------

TITLE: Run Model via Ollama URI
DESCRIPTION: Installs and runs a model from the Ollama registry.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#_snippet_3

LANGUAGE: bash
CODE:
```
local-ai run ollama://gemma:2b
```

----------------------------------------

TITLE: LocalAI Docker Run Example
DESCRIPTION: This snippet demonstrates how to run LocalAI using a Docker container. It maps a local models directory to the container, exposes the API port, and specifies model path and resource configurations. The API becomes accessible at localhost:8080.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#_snippet_8

LANGUAGE: bash
CODE:
```
cp -rf prompt-templates/getting_started.tmpl models/luna-ai-llama2.tmpl
docker run -p 8080:8080 -v $PWD/models:/models -ti --rm quay.io/go-skynet/local-ai:latest --models-path /models --context-size 700 --threads 4
```

----------------------------------------

TITLE: Track Model Installation Job
DESCRIPTION: After initiating a model installation, the API returns a job UUID. This UUID can be used to poll the status of the installation job. The example uses `jq` to parse the response and check the `processed` status.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#_snippet_10

LANGUAGE: bash
CODE:
```
response=$(curl -s http://localhost:8080/models/apply -H "Content-Type: application/json" -d '{"url": "$model_url"}')

job_id=$(echo "$response" | jq -r '.uuid')

while [ "$(curl -s http://localhost:8080/models/jobs/"$job_id" | jq -r '.processed')" != "true" ]; do 
  sleep 1
done

echo "Job completed"
```

----------------------------------------

TITLE: Install LocalAI
DESCRIPTION: Installs LocalAI using a curl command. The installation can be customized using environment variables passed during the execution.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/installer.md#_snippet_0

LANGUAGE: bash
CODE:
```
curl https://localai.io/install.sh | sh
```

----------------------------------------

TITLE: Download Example Audio and Transcribe
DESCRIPTION: This example demonstrates downloading an audio file using wget and then sending it to the LocalAI transcription endpoint using cURL. It shows the complete process from obtaining audio data to receiving the transcription result.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/audio-to-text.md#_snippet_2

LANGUAGE: bash
CODE:
```
## Get an example audio file
curl -s --show-progress -o gb1.ogg https://upload.wikimedia.org/wikipedia/commons/1/1f/George_W_Bush_Columbia_FINAL.ogg

## Send the example audio file to the transcriptions endpoint
curl -s http://localhost:8080/v1/audio/transcriptions -H "Content-Type: multipart/form-data" -F file="@$PWD/gb1.ogg" -F model="whisper-1"

## Result
{"text":"My fellow Americans, this day has brought terrible news and great sadness to our country.At nine o'clock this morning, Mission Control in Houston lost contact with our Space ShuttleColumbia.A short time later, debris was seen falling from the skies above Texas.The Columbia's lost.There are no survivors.One board was a crew of seven.Colonel Rick Husband, Lieutenant Colonel Michael Anderson, Commander Laurel Clark, Captain DavidBrown, Commander William McCool, Dr. Kultna Shavla, and Elon Ramon, a colonel in the IsraeliAir Force.These men and women assumed great risk in the service to all humanity.In an age when spaceflight has come to seem almost routine, it is easy to overlook thedangers of travel by rocket and the difficulties of navigating the fierce outer atmosphere ofthe Earth.These astronauts knew the dangers, and they faced them willingly, knowing they had a highand noble purpose in life.Because of their courage and daring and idealism, we will miss them all the more.All Americans today are thinking as well of the families of these men and women who havebeen given this sudden shock and grief.You're not alone.Our entire nation agrees with you, and those you loved will always have the respect andgratitude of this country.The cause in which they died will continue.Mankind has led into the darkness beyond our world by the inspiration of discovery andthe longing to understand.Our journey into space will go on.In the skies today, we saw destruction and tragedy.As farther than we can see, there is comfort and hope.In the words of the prophet Isaiah, \"Lift your eyes and look to the heavens who createdall these, he who brings out the starry hosts one by one and calls them each by name.\"Because of his great power and mighty strength, not one of them is missing.The same creator who names the stars also knows the names of the seven souls we mourntoday.The crew of the shuttle Columbia did not return safely to Earth yet we can pray that all aresafely home.May God bless the grieving families and may God continue to bless America.[BLANK_AUDIO]"}
```

----------------------------------------

TITLE: PyTorch with ROCm Support Installation
DESCRIPTION: Installs PyTorch and Torchvision with ROCm 6.0 support, along with other essential libraries for LocalAI. This command is typically used in a Python environment setup.

SOURCE: https://github.com/mudler/localai/blob/master/backend/python/diffusers/requirements-hipblas.txt#_snippet_0

LANGUAGE: shell
CODE:
```
--extra-index-url https://download.pytorch.org/whl/rocm6.0
torch==2.3.1+rocm6.0
torchvision==0.18.1+rocm6.0
diffusers
opencv-python
transformers
accelerate
compel
peft
sentencepiece
optimum-quanto
```

----------------------------------------

TITLE: Pre-install Backends
DESCRIPTION: Shows how to pre-install backends when starting LocalAI by setting the LOCALAI_EXTERNAL_BACKENDS environment variable with a comma-separated list of backend names.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/backends.md#_snippet_2

LANGUAGE: bash
CODE:
```
export LOCALAI_EXTERNAL_BACKENDS="llm-backend,diffusion-backend"
local-ai run
```

----------------------------------------

TITLE: Run Model via OCI Registry URI
DESCRIPTION: Installs and runs a model from a standard OCI registry like Docker Hub.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#_snippet_5

LANGUAGE: bash
CODE:
```
local-ai run oci://localai/phi-2:latest
```

----------------------------------------

TITLE: NVIDIA CUDA GPU Acceleration Setup
DESCRIPTION: Instructions for setting up NVIDIA GPU acceleration with LocalAI. This involves installing the nvidia-container-toolkit, ensuring correct CUDA versions, and running Docker containers with GPU support.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/GPU-acceleration.md#_snippet_3

LANGUAGE: bash
CODE:
```
docker run --runtime=nvidia --rm nvidia/cuda:12.8.0-base-ubuntu24.04 nvidia-smi
```

LANGUAGE: bash
CODE:
```
docker run --rm -ti --gpus all -p 8080:8080 -e DEBUG=true -e MODELS_PATH=/models -e THREADS=1 -v $PWD/models:/models quay.io/go-skynet/local-ai:v1.40.0-gpu-nvidia-cuda12
```

----------------------------------------

TITLE: Run Model from Gallery
DESCRIPTION: Executes LocalAI with a model specified by its name from the LocalAI gallery.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#_snippet_0

LANGUAGE: bash
CODE:
```
local-ai run hermes-2-theta-llama-3-8b
```

----------------------------------------

TITLE: Configure Installation with Environment Variables
DESCRIPTION: Demonstrates how to configure the LocalAI installation using environment variables. This allows for customization of Docker image usage, GPU support, API keys, port, threads, version, model paths, and distributed inferencing settings.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/installer.md#_snippet_1

LANGUAGE: bash
CODE:
```
curl https://localai.io/install.sh | VAR=value sh
```

----------------------------------------

TITLE: Install Dependencies on Apple (macOS)
DESCRIPTION: Installs Go, protobuf, and related gRPC tools using Homebrew on macOS. These are essential for building LocalAI and its components.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#_snippet_0

LANGUAGE: bash
CODE:
```
brew install go protobuf protoc-gen-go protoc-gen-go-grpc wget
```

----------------------------------------

TITLE: Build Backend from Source (bark-cpp example)
DESCRIPTION: Demonstrates how to build a specific backend, `bark-cpp`, from its source code within the LocalAI repository using a Makefile.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#_snippet_6

LANGUAGE: shell
CODE:
```
git clone https://github.com/go-skynet/LocalAI.git

# Build the bark-cpp backend (requires cmake)
make -C LocalAI/backend/go/bark-cpp build package
```

----------------------------------------

TITLE: Start Federated Instance (CLI)
DESCRIPTION: Starts a new LocalAI instance for sharing in a federated network. Requires a P2P token.

SOURCE: https://github.com/mudler/localai/blob/master/core/http/views/p2p.html#_snippet_0

LANGUAGE: bash
CODE:
```
export TOKEN="{{.P2PToken}}"
local-ai run --federated --p2p
```

----------------------------------------

TITLE: LocalAI Function Calling Example
DESCRIPTION: Illustrates the concept of running OpenAI functions locally with LocalAI, referencing external blog posts and documentation for detailed explanations. It highlights the ability to either use functions or reply directly to the user.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/whats-new.md#_snippet_38

LANGUAGE: go
CODE:
```
// This is a conceptual example and does not represent actual runnable code from the provided text.
// The text describes the functionality and provides links to examples.

// Example of how LocalAI might handle function calls (conceptual):
// func HandleRequest(requestData string) {
//   // Parse requestData to identify if function calling is intended
//   // If function calling is intended:
//   //   Call appropriate LocalAI function execution logic
//   // Else:
//   //   Process as a standard text generation request
// }

// The actual implementation details are within the LocalAI project's codebase.
// Refer to: https://openai.com/blog/function-calling-and-other-api-updates
```

----------------------------------------

TITLE: Install LocalAI Dependencies
DESCRIPTION: This snippet shows the pip installation command with an extra index URL for Intel-specific PyTorch extensions. It includes PyTorch, Intel Extension for PyTorch, Torchaudio, oneCCL, Optimum with OpenVINO support, setuptools, transformers, and accelerate.

SOURCE: https://github.com/mudler/localai/blob/master/backend/python/bark/requirements-intel.txt#_snippet_0

LANGUAGE: shell
CODE:
```
pip install \
    --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/ \
    intel-extension-for-pytorch==2.3.110+xpu \
    torch==2.3.1+cxx11.abi \
    torchaudio==2.3.1+cxx11.abi \
    oneccl_bind_pt==2.3.100+xpu \
    optimum[openvino] \
    setuptools \
    transformers \
    accelerate
```

----------------------------------------

TITLE: Start Fine-tuning with Axolotl
DESCRIPTION: Initiates the fine-tuning process using Axolotl by launching the training script with the specified configuration file.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/fine-tuning.md#_snippet_3

LANGUAGE: bash
CODE:
```
# Fine-tune
accelerate launch -m axolotl.cli.train axolotl.yaml
```

----------------------------------------

TITLE: LocalAI Installer Environment Variables
DESCRIPTION: Lists and describes the environment variables available for configuring the LocalAI installation script. These variables control aspects like Docker image usage, GPU support, API key, port, threads, version, model paths, and distributed inferencing.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/installer.md#_snippet_3

LANGUAGE: APIDOC
CODE:
```
Environment Variables:

- DOCKER_INSTALL: Set to "true" to enable the installation of Docker images.
- USE_AIO: Set to "true" to use the all-in-one LocalAI Docker image.
- USE_VULKAN: Set to "true" to use Vulkan GPU support.
- API_KEY: Specify an API key for accessing LocalAI, if required.
- PORT: Specifies the port on which LocalAI will run (default is 8080).
- THREADS: Number of processor threads the application should use. Defaults to the number of logical cores minus one.
- VERSION: Specifies the version of LocalAI to install. Defaults to the latest available version.
- MODELS_PATH: Directory path where LocalAI models are stored (default is /usr/share/local-ai/models).
- P2P_TOKEN: Token to use for the federation or for starting workers see documentation.
- WORKER: Set to "true" to make the instance a worker (p2p token is required see documentation).
- FEDERATED: Set to "true" to share the instance with the federation (p2p token is required see documentation).
- FEDERATED_SERVER: Set to "true" to run the instance as a federation server which forwards requests to the federation (p2p token is required see documentation).
```

----------------------------------------

TITLE: Install Model using Configuration URL
DESCRIPTION: Installs a model by providing a direct URL to its configuration file. This method is used when the model is not part of a gallery. The `config_url` or `url` field specifies the location of the model configuration.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#_snippet_8

LANGUAGE: bash
CODE:
```
LOCALAI=http://localhost:8080
curl $LOCALAI/models/apply -H "Content-Type: application/json" -d '{
     "config_url": "<MODEL_CONFIG_FILE_URL>"
   }'
```

LANGUAGE: bash
CODE:
```
LOCALAI=http://localhost:8080
curl $LOCALAI/models/apply -H "Content-Type: application/json" -d '{
     "url": "<MODEL_CONFIG_FILE_URL>"
   }'
```

LANGUAGE: bash
CODE:
```
LOCALAI=http://localhost:8080
curl $LOCALAI/models/apply -H "Content-Type: application/json" -d '{
     "config_url": "https://raw.githubusercontent.com/mudler/LocalAI/v2.25.0/embedded/models/hermes-2-pro-mistral.yaml"
   }'
```

----------------------------------------

TITLE: Install Model from Gallery
DESCRIPTION: Installs a model from the LocalAI gallery by specifying its identifier. The `id` field uses the format `repository@model_name`. If the repository is omitted, LocalAI searches all repositories.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#_snippet_7

LANGUAGE: bash
CODE:
```
LOCALAI=http://localhost:8080
curl $LOCALAI/models/apply -H "Content-Type: application/json" -d '{
     "id": "localai@bert-embeddings"
   }'
```

----------------------------------------

TITLE: Install Axolotl and Dependencies
DESCRIPTION: Installs the Axolotl library and its dependencies, including flash-attention, for fine-tuning language models. It also includes instructions for configuring the 'accelerate' library.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/fine-tuning.md#_snippet_1

LANGUAGE: bash
CODE:
```
# Install axolotl and dependencies
git clone https://github.com/OpenAccess-AI-Collective/axolotl && pushd axolotl && git checkout 797f3dd1de8fd8c0eafbd1c9fdb172abd9ff840a && popd #0.3.0
pip install packaging
pushd axolotl && pip install -e '.[flash-attn,deepspeed]' && popd

# https://github.com/oobabooga/text-generation-webui/issues/4238
pip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.3.0/flash_attn-2.3.0+cu117torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl
```

LANGUAGE: bash
CODE:
```
# Configure accelerate
accelerate config default
```

----------------------------------------

TITLE: Install and Run Model from OCI Registry
DESCRIPTION: This command demonstrates how to install and run a language model from a standard OCI registry, such as Docker Hub. It utilizes the `local-ai run` command with the OCI path to the model.

SOURCE: https://github.com/mudler/localai/blob/master/README.md#_snippet_16

LANGUAGE: bash
CODE:
```
local-ai run oci://localai/phi-2:latest
```

----------------------------------------

TITLE: Build vllm Backend
DESCRIPTION: Builds the vllm backend for LocalAI, which requires Python to be installed.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/build.md#_snippet_7

LANGUAGE: bash
CODE:
```
make -C LocalAI/backend/python/vllm
```

----------------------------------------

TITLE: Download Model for Local Deployment
DESCRIPTION: Provides a command to download a specific model file (e.g., Luna AI Llama2) for manual deployment.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#_snippet_7

LANGUAGE: bash
CODE:
```
mkdir models

# Download luna-ai-llama2 to models/ 
wget https://huggingface.co/TheBloke/Luna-AI-Llama2-Uncensored-GGUF/resolve/main/luna-ai-llama2-uncensored.Q4_0.gguf -O models/luna-ai-llama2
```

----------------------------------------

TITLE: Displaying Models Without Configuration
DESCRIPTION: Lists models that have been installed but do not have a corresponding configuration file, indicating they are set to 'auto' for backend selection.

SOURCE: https://github.com/mudler/localai/blob/master/core/http/views/index.html#_snippet_1

LANGUAGE: html
CODE:
```
{{ range .Models }}

  ![Model icon]({{$noicon}})

  ### {{.}}

  auto No Configuration

{{ end }}
```

----------------------------------------

TITLE: LocalAI Docker Installation (Intel GPU - oneAPI)
DESCRIPTION: Runs a LocalAI container with Intel GPU support using oneAPI.

SOURCE: https://github.com/mudler/localai/blob/master/README.md#_snippet_8

LANGUAGE: docker
CODE:
```
# Intel GPU Images (oneAPI)
docker run -ti --name local-ai -p 8080:8080 --device=/dev/dri/card1 --device=/dev/dri/renderD128 localai/localai:latest-gpu-intel
```

----------------------------------------

TITLE: LocalAI API: List Models
DESCRIPTION: This API call retrieves a list of available models from the LocalAI instance. It expects a JSON response containing model details.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/models.md#_snippet_9

LANGUAGE: APIDOC
CODE:
```
GET /v1/models
Response:
{"object":"list","data":[{"id":"luna-ai-llama2","object":"model"}]}
```

----------------------------------------

TITLE: Exllama Model Setup and Usage
DESCRIPTION: Demonstrates how to set up and use the Exllama backend for quantized models in LocalAI. This includes cloning a model, creating a configuration file, and making a sample API request.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/text-generation.md#_snippet_6

LANGUAGE: bash
CODE:
```
$ git lfs install
$ cd models && git clone https://huggingface.co/TheBloke/WizardLM-7B-uncensored-GPTQ
$ ls models/                                                                 
.keep                        WizardLM-7B-uncensored-GPTQ/ exllama.yaml
$ cat models/exllama.yaml                                                     
name: exllama
parameters:
  model: WizardLM-7B-uncensored-GPTQ
backend: exllama
# Note: you can also specify "exllama2" if it's an exllama2 model here
# ...

```

LANGUAGE: json
CODE:
```
{
   "model": "exllama",
   "messages": [{"role": "user", "content": "How are you?"}],
   "temperature": 0.1
 }
```

LANGUAGE: bash
CODE:
```
curl http://localhost:8080/v1/chat/completions -H "Content-Type: application/json" -d '{                                                                                                         
   "model": "exllama",
   "messages": [{"role": "user", "content": "How are you?"}],
   "temperature": 0.1
 }'
```

----------------------------------------

TITLE: Install LocalAI Dependencies
DESCRIPTION: This Python script installs the necessary libraries for the LocalAI project, including PyTorch, transformers, and bitsandbytes.

SOURCE: https://github.com/mudler/localai/blob/master/backend/python/vllm/requirements-cublas11.txt#_snippet_1

LANGUAGE: python
CODE:
```
import subprocess

def install_dependencies():
    dependencies = [
        "--extra-index-url https://download.pytorch.org/whl/cu118",
        "accelerate",
        "torch==2.7.0+cu118",
        "transformers",
        "bitsandbytes"
    ]
    try:
        subprocess.check_call(["pip", "install"] + dependencies)
        print("Dependencies installed successfully!")
    except subprocess.CalledProcessError as e:
        print(f"Error installing dependencies: {e}")

if __name__ == "__main__":
    install_dependencies()

```

----------------------------------------

TITLE: Start Federated Instance (Docker)
DESCRIPTION: Starts a new LocalAI instance for sharing in a federated network using Docker. Requires a P2P token.

SOURCE: https://github.com/mudler/localai/blob/master/core/http/views/p2p.html#_snippet_2

LANGUAGE: dockerfile
CODE:
```
docker run -ti --net host -e TOKEN="{{.P2PToken}}" --name local-ai -p 8080:8080 localai/localai:latest-cpu run --federated --p2p
```

----------------------------------------

TITLE: Start llama.cpp P2P Worker (CLI)
DESCRIPTION: Starts a llama.cpp worker for P2P distributed inference using the CLI. Requires a P2P token.

SOURCE: https://github.com/mudler/localai/blob/master/core/http/views/p2p.html#_snippet_4

LANGUAGE: bash
CODE:
```
export TOKEN="{{.P2PToken}}"
local-ai worker p2p-llama-cpp-rpc
```

----------------------------------------

TITLE: Start Federated Load Balancer (CLI)
DESCRIPTION: Starts a LocalAI federated server to balance requests between nodes. Requires a P2P token.

SOURCE: https://github.com/mudler/localai/blob/master/core/http/views/p2p.html#_snippet_1

LANGUAGE: bash
CODE:
```
export TOKEN="{{.P2PToken}}"
local-ai federated
```

----------------------------------------

TITLE: Prompt Template Example for Alpaca Models
DESCRIPTION: An example of a prompt template for Alpaca-based models, used to format instructions and inputs for the model. This template includes placeholders for the instruction and input, and is typically placed in a file with a `.tmpl` suffix next to the model file.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#_snippet_16

LANGUAGE: yaml
CODE:
```
The below instruction describes a task. Write a response that appropriately completes the request.

### Instruction:
{{.Input}}

### Response:
```

----------------------------------------

TITLE: LocalAI Docker Installation (AIO CPU)
DESCRIPTION: Runs an AIO LocalAI container with CPU support and pre-downloaded models.

SOURCE: https://github.com/mudler/localai/blob/master/README.md#_snippet_10

LANGUAGE: docker
CODE:
```
# CPU version
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-cpu
```

----------------------------------------

TITLE: Get Model Job State
DESCRIPTION: Retrieves the status of a model installation job using its unique job ID. Returns information about whether the job has been processed, any errors encountered, and a status message.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/features/model-gallery.md#_snippet_32

LANGUAGE: bash
CODE:
```
curl http://localhost:8080/models/jobs/<JOB_ID>
```

LANGUAGE: json
CODE:
```
{"error":null,"processed":true,"message":"completed"}
```

----------------------------------------

TITLE: Install Intel Extension for PyTorch
DESCRIPTION: This command installs the Intel Extension for PyTorch along with specific versions of PyTorch and related libraries. It uses an extra index URL to fetch the correct packages for Intel XPU hardware.

SOURCE: https://github.com/mudler/localai/blob/master/backend/python/kokoro/requirements-intel.txt#_snippet_0

LANGUAGE: bash
CODE:
```
--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
intel-extension-for-pytorch==2.3.110+xpu
torch==2.3.1+cxx11.abi
oneccl_bind_pt==2.3.100+xpu
```

----------------------------------------

TITLE: Install Packages in Conda Environment
DESCRIPTION: Installs packages into the active conda environment. Packages can be installed directly using 'conda install' or from specific channels like 'conda-forge'. Alternatively, 'pip install' can be used for packages available on PyPI.

SOURCE: https://github.com/mudler/localai/blob/master/backend/python/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
# Using conda
conda install <your-package-name>

conda install -c conda-forge <your package-name>

# Using pip
pip install <your-package-name>
```

----------------------------------------

TITLE: LocalAI Docker Installation (CPU)
DESCRIPTION: Runs a LocalAI container on CPU.

SOURCE: https://github.com/mudler/localai/blob/master/README.md#_snippet_3

LANGUAGE: docker
CODE:
```
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest
```

----------------------------------------

TITLE: LocalAI API: Model Installation and Management
DESCRIPTION: Documentation for the LocalAI API endpoints related to model management. This includes applying models from a URL and managing preloaded models during startup. It details the request format for applying models and the environment variables for preloading.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/advanced/advanced-usage.md#_snippet_22

LANGUAGE: APIDOC
CODE:
```
POST /models/apply
  Applies a model to LocalAI from a given URL.
  Request Body:
    id: string (required) - The identifier for the model, often a path or URL to the model file.
    name: string (optional) - The desired name for the model within LocalAI.
  Example:
    curl --location 'http://localhost:8080/models/apply' \
    --header 'Content-Type: application/json' \
    --data-raw '{ "id": "TheBloke/Luna-AI-Llama2-Uncensored-GGML/luna-ai-llama2-uncensored.ggmlv3.q5_K_M.bin", "name": "lunademo" }'

Environment Variables for Startup:
  PRELOAD_MODELS: string (optional) - A JSON array of model definitions to preload on startup. Each definition should have 'url' and optionally 'name'.
    Example: '[{"url": "https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml","name": "gpt4all-j"}]'
  PRELOAD_MODELS_CONFIG: string (optional) - Path to a YAML configuration file containing a list of models to preload.
    Example YAML:
    - url: https://raw.githubusercontent.com/go-skynet/model-gallery/main/gpt4all-j.yaml
      name: gpt4all-j
```

----------------------------------------

TITLE: Start llama.cpp P2P Worker (Docker)
DESCRIPTION: Starts a llama.cpp worker for P2P distributed inference using Docker. Requires a P2P token.

SOURCE: https://github.com/mudler/localai/blob/master/core/http/views/p2p.html#_snippet_5

LANGUAGE: dockerfile
CODE:
```
docker run -ti --net host -e TOKEN="{{.P2PToken}}" --name local-ai -p 8080:8080 localai/localai:latest-cpu worker p2p-llama-cpp-rpc
```

----------------------------------------

TITLE: LocalAI Docker Installation (AIO Intel GPU)
DESCRIPTION: Runs an AIO LocalAI container with Intel GPU support and pre-downloaded models.

SOURCE: https://github.com/mudler/localai/blob/master/README.md#_snippet_13

LANGUAGE: docker
CODE:
```
# Intel GPU version
docker run -ti --name local-ai -p 8080:8080 localai/localai:latest-aio-gpu-intel
```

----------------------------------------

TITLE: Initiating LocalAI with a Specific Model Configuration
DESCRIPTION: Shows how to run the LocalAI Docker container and load a specific model configuration file from a URL. This example uses the phi-2 model configuration.

SOURCE: https://github.com/mudler/localai/blob/master/docs/content/docs/getting-started/customize-model.md#_snippet_1

LANGUAGE: bash
CODE:
```
docker run -p 8080:8080 localai/localai:{{< version >}} https://gist.githubusercontent.com/mudler/ad601a0488b497b69ec549150d9edd18/raw/a8a8869ef1bb7e3830bf5c0bae29a0cce991ff8d/phi-2.yaml
```

----------------------------------------

TITLE: Install LocalAI Dependencies
DESCRIPTION: This snippet shows the pip install command with extra index URLs and specific package versions required for the LocalAI project. It includes PyTorch with Intel XPU extensions and Hugging Face transformers.

SOURCE: https://github.com/mudler/localai/blob/master/backend/python/coqui/requirements-intel.txt#_snippet_0

LANGUAGE: shell
CODE:
```
# Install PyTorch with Intel XPU support
--extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/
intel-extension-for-pytorch==2.3.110+xpu
torch==2.3.1+cxx11.abi
torchaudio==2.3.1+cxx11.abi
oneccl_bind_pt==2.3.100+xpu

# Install other project dependencies
optimum[openvino]
setuptools
transformers==4.48.3
accelerate
coqui-tts
```